#!/usr/bin/env python3
"""
é€šè¿‡deepseek APIç”Ÿæˆå®Œæ•´çš„Merlin Chain function callingæ•°æ®é›†
åŸºäºå·²ç”Ÿæˆçš„é—®é¢˜æ•°æ®ï¼Œè¡¥å…¨assistantçš„å“åº”å’Œfunction calling
"""
import asyncio
import argparse
import json
import logging
import re
import time
import signal
import os
from typing import List, Dict, Any
from deepseek_api_client import DeepSeekAPIClient
from dataset_utils import DatasetUtils
from merlin_mcp_client import MerlinMCPClient
from config_manager import ConfigManager


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('complete_dataset.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CompleteDatasetGenerator:
    """å®Œæ•´æ•°æ®é›†ç”Ÿæˆå™¨ï¼Œä¸ºé—®é¢˜æ•°æ®è¡¥å…¨assistantå“åº”"""
    
    def __init__(self, config_manager: ConfigManager):
        self.config = config_manager
        api_key = self.config.get_api_key()
        if not api_key:
            raise ValueError("API Keyæœªé…ç½®ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡")
        
        self.api_client = DeepSeekAPIClient(api_key)
        self.utils = DatasetUtils()
        self.mcp_client = MerlinMCPClient()
        
        # ä»é…ç½®è·å–ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶
        prompt_file = self.config.get('completion.system_prompt_file', 'prompt-2.txt')
        self.system_prompt = self._load_system_prompt(prompt_file)
        logger.info("CompleteDatasetGenerator åˆå§‹åŒ–å®Œæˆ")
    
    def _load_system_prompt(self, prompt_file: str) -> str:
        """åŠ è½½ç³»ç»Ÿæç¤ºè¯"""
        try:
            system_prompt = self.utils.read_file(prompt_file)
            if not system_prompt:
                logger.warning(f"æ— æ³•è¯»å– {prompt_file}ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯")
                system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Merlin ChainåŠ©æ‰‹ï¼Œæ“…é•¿ä½¿ç”¨å„ç§MCPå·¥å…·å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢é“¾ä¸Šæ•°æ®ã€‚"
            logger.info(f"å·²åŠ è½½ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶: {prompt_file}ï¼Œé•¿åº¦: {len(system_prompt)} å­—ç¬¦")
            return system_prompt
        except Exception as e:
            logger.error(f"åŠ è½½ç³»ç»Ÿæç¤ºè¯å¤±è´¥: {e}")
            return "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Merlin ChainåŠ©æ‰‹ï¼Œæ“…é•¿ä½¿ç”¨å„ç§MCPå·¥å…·å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢é“¾ä¸Šæ•°æ®ã€‚"
    
    async def _get_mcp_tools_schema(self) -> List[Dict]:
        """è·å–MCPå·¥å…·çš„JSON Schemaæ ¼å¼"""
        try:
            if not self.mcp_client.connected:
                await self.mcp_client.connect()
            
            tools_info = self.mcp_client.get_all_tools_info()
            tools_schema = []
            
            for tool_name, tool_info in tools_info.items():
                # è½¬æ¢ä¸ºOpenAI function callingæ ¼å¼
                tool_schema = {
                    "type": "function",
                    "function": {
                        "name": f"mcp__merlin_mcp_tool__{tool_name}",
                        "description": tool_info.description,
                        "parameters": tool_info.parameters
                    }
                }
                tools_schema.append(tool_schema)
            
            logger.info(f"è·å–åˆ° {len(tools_schema)} ä¸ªMCPå·¥å…·å®šä¹‰")
            return tools_schema
            
        except Exception as e:
            logger.error(f"è·å–MCPå·¥å…·å®šä¹‰å¤±è´¥: {e}")
            return []
    
    def _create_completion_prompt(self, conversations: List[Dict], tools_definition: str) -> str:
        """åˆ›å»ºè¡¥å…¨prompt"""
        # æå–å¯¹è¯å†å²
        dialog_history = []
        for conv in conversations:
            role = conv.get("from", "")
            content = conv.get("value", "")
            if role and content:
                # ç»Ÿä¸€è§’è‰²åç§°
                if role == "user":
                    role = "human"
                dialog_history.append(f"{role}: {content}")
        
        dialog_text = "\n".join(dialog_history)
        
        completion_prompt = f"""
{tools_definition}

=== ä»»åŠ¡è¯´æ˜ ===
è¯·åŸºäºä»¥ä¸‹å¯¹è¯å†å²ï¼Œç”Ÿæˆå®Œæ•´çš„function callingå¯¹è¯ã€‚éœ€è¦åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š

1. function_call: è°ƒç”¨åˆé€‚çš„MCPå·¥å…·
2. observation: å·¥å…·è¿”å›çš„ç»“æœ
3. gpt: åŸºäºå·¥å…·ç»“æœç»™ç”¨æˆ·çš„æœ€ç»ˆå›å¤

å¯¹è¯å†å²ï¼š
{dialog_text}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”Ÿæˆå®Œæ•´çš„å¯¹è¯ï¼š
1. æ·»åŠ  "from": "function_call" çš„æ¶ˆæ¯ï¼ŒåŒ…å«å·¥å…·è°ƒç”¨
2. æ·»åŠ  "from": "observation" çš„æ¶ˆæ¯ï¼ŒåŒ…å«å·¥å…·è¿”å›ç»“æœ
3. æ·»åŠ  "from": "gpt" çš„æ¶ˆæ¯ï¼ŒåŒ…å«æœ€ç»ˆç”¨æˆ·å›å¤

æ ¼å¼è¦æ±‚ï¼š
- function_callçš„valueæ˜¯JSONå­—ç¬¦ä¸²ï¼š{{"name": "å·¥å…·å", "arguments": {{å‚æ•°}}}}
- observationçš„valueæ˜¯å·¥å…·è¿”å›çš„JSONç»“æœ
- gptçš„valueæ˜¯ç”¨æˆ·å‹å¥½çš„å›å¤æ–‡æœ¬

è¯·åªè¿”å›éœ€è¦æ·»åŠ çš„å¯¹è¯æ¶ˆæ¯ï¼Œä¸è¦é‡å¤å·²æœ‰çš„å¯¹è¯å†å²ã€‚
"""
        return completion_prompt
    
    def _parse_completion_response(self, response: str) -> List[Dict]:
        """è§£æAPIè¿”å›çš„è¡¥å…¨å“åº”ï¼Œæå–function_callã€observationã€gptæ¶ˆæ¯"""
        logger.info("=== å¤§æ¨¡å‹åŸå§‹è¿”å› ===\n" + response + "\n=== ç»“æŸ ===")
        
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            try:
                data = json.loads(response)
                if isinstance(data, list):
                    logger.info(f"æˆåŠŸè§£æJSONæ ¼å¼ï¼ŒåŒ…å« {len(data)} ä¸ªæ¶ˆæ¯")
                    return data
            except json.JSONDecodeError:
                pass
            
            # å°è¯•ä»æ–‡æœ¬æå–JSONä»£ç å—
            json_blocks = re.findall(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_blocks:
                try:
                    data = json.loads(json_blocks[0])
                    if isinstance(data, list):
                        logger.info(f"ä»ä»£ç å—è§£ææˆåŠŸï¼ŒåŒ…å« {len(data)} ä¸ªæ¶ˆæ¯")
                        return data
                except json.JSONDecodeError:
                    pass
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æ„åŒ–çš„æ¶ˆæ¯ï¼Œå°è¯•ç”Ÿæˆé»˜è®¤æ ¼å¼
            logger.warning("æ— æ³•è§£æç»“æ„åŒ–å“åº”ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼")
            messages = self._generate_default_messages(response)
            return messages
            
        except Exception as e:
            logger.error(f"è§£æå“åº”å¤±è´¥: {e}")
            return []

    
    def _generate_default_messages(self, response: str) -> List[Dict]:
        """ç”Ÿæˆé»˜è®¤çš„æ¶ˆæ¯æ ¼å¼ï¼ˆå¤‡ç”¨ï¼‰"""
        return [
            {
                "from": "function_call",
                "value": '{"name": "search_chain_data", "arguments": {"query": "æŸ¥è¯¢"}}'
            },
            {
                "from": "observation", 
                "value": '{"content": [{"type": "text", "text": "{\\"results\\": [{\\"	ype\\": \\"info\\", \\"message\\": \\"æŸ¥è¯¢ç»“æœ\\"}"}]}'
            },
            {
                "from": "gpt",
                "value": response.strip()
            }
        ]
    
    def _create_gpt_response_prompt(self, conversations: List[Dict], mcp_messages: List[Dict]) -> str:
        """åˆ›å»ºç”ŸæˆGPTå›å¤çš„prompt"""
        # æå–ç”¨æˆ·é—®é¢˜
        user_question = ""
        for conv in conversations:
            if conv.get("from") == "user":
                user_question = conv.get("value", "")
                break
        
        # æå–MCPè°ƒç”¨ç»“æœ
        observation_data = ""
        for msg in mcp_messages:
            if msg.get("from") == "observation":
                observation_data = msg.get("value", "")
                break
        
        prompt = f"""
ç”¨æˆ·é—®é¢˜: {user_question}

MCPå·¥å…·è°ƒç”¨ç»“æœ: {observation_data}

è¯·åŸºäºä¸Šè¿°å·¥å…·è°ƒç”¨ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·å‹å¥½çš„å›å¤ã€‚è¦æ±‚ï¼š
1. è§£é‡ŠæŸ¥è¯¢åˆ°çš„æ•°æ®
2. æä¾›æœ‰ä»·å€¼çš„åˆ†æå’Œæ€»ç»“
3. ä½¿ç”¨æ¸…æ™°æ˜“æ‡‚çš„è¯­è¨€
4. å¦‚æœæ˜¯äº¤æ˜“è®°å½•ï¼ŒæŒ‰æ—¶é—´é¡ºåºåˆ—å‡º
5. å¦‚æœæ˜¯æ•°æ®åˆ†æï¼Œæä¾›å…³é”®æ´å¯Ÿ

è¯·ç›´æ¥è¿”å›å›å¤å†…å®¹ï¼Œä¸éœ€è¦å…¶ä»–æ ¼å¼ã€‚
"""
        return prompt
    
    def _get_tools_json(self) -> str:
        """è·å–å·¥å…·å®šä¹‰çš„JSONå­—ç¬¦ä¸²"""
        try:
            tools_info = self.mcp_client.get_all_tools_info()
            tools_list = []
            
            for tool_name, tool_info in tools_info.items():
                tool_def = {
                    "name": f"mcp__merlin_mcp_tool__{tool_name}",
                    "description": tool_info.description,
                    "parameters": tool_info.parameters
                }
                tools_list.append(tool_def)
            
            return json.dumps(tools_list, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå·¥å…·JSONå¤±è´¥: {e}")
            return "[]"

    async def _handle_interruption(self, all_completed: List[Dict], temp_files: List[str], output_file: str, start_from: int):
        """å¤„ç†ç¨‹åºä¸­æ–­ï¼Œåˆå¹¶ä¸´æ—¶æ–‡ä»¶å¹¶ä»¥å¢é‡å½¢å¼ä¿å­˜åˆ°é»˜è®¤è¾“å‡ºæ–‡ä»¶"""
        try:
            logger.info("æ­£åœ¨å¤„ç†ç¨‹åºä¸­æ–­...")
            
            # åªåˆå¹¶ä¸´æ—¶æ–‡ä»¶ä¸­çš„æ•°æ®ï¼Œå¿½ç•¥å†…å­˜ä¸­çš„æ•°æ®
            temp_data = []
            for temp_file in temp_files:
                if os.path.exists(temp_file):
                    try:
                        temp_content = self.utils.load_json_file(temp_file)
                        if temp_content:
                            temp_data.extend(temp_content)
                            logger.info(f"å·²åŠ è½½ä¸´æ—¶æ–‡ä»¶: {temp_file}, {len(temp_content)} æ¡è®°å½•")
                    except Exception as e:
                        logger.error(f"åŠ è½½ä¸´æ—¶æ–‡ä»¶ {temp_file} å¤±è´¥: {e}")
            
            if temp_data:
                # è¯»å–ç°æœ‰è¾“å‡ºæ–‡ä»¶çš„æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                existing_data = []
                if os.path.exists(output_file):
                    try:
                        existing_data = self.utils.load_json_file(output_file)
                        if existing_data:
                            logger.info(f"å·²è¯»å–ç°æœ‰è¾“å‡ºæ–‡ä»¶: {output_file}, {len(existing_data)} æ¡è®°å½•")
                    except Exception as e:
                        logger.warning(f"è¯»å–ç°æœ‰è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
                
                # åˆå¹¶æ•°æ®ï¼šç°æœ‰æ•°æ® + ä¸´æ—¶æ–‡ä»¶æ•°æ®
                final_data = existing_data + temp_data
                
                # ä¿å­˜åˆ°é»˜è®¤è¾“å‡ºæ–‡ä»¶
                self.utils.write_json_file(final_data, output_file)
                logger.info(f"ğŸ’¾ ç¨‹åºä¸­æ–­ï¼Œå·²å°† {len(temp_data)} ä¸ªä¸´æ—¶æ–‡ä»¶è®°å½•åˆå¹¶åˆ°: {output_file}")
                logger.info(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶ç°æœ‰è®°å½•æ•°: {len(final_data)} (åŸæœ‰ {len(existing_data)} + æ–°å¢ {len(temp_data)})")
                
                # è®¡ç®—ç»§ç»­å¤„ç†çš„å»ºè®®èµ·å§‹ä½ç½®
                next_start_from = start_from + len(temp_data)
                logger.info(f"ğŸ’¡ å»ºè®®ç»­ä¼ å‚æ•°: --start_from {next_start_from}")
                print(f"ğŸ’¾ ç¨‹åºä¸­æ–­ï¼Œå·²ä¿å­˜ {len(temp_data)} ä¸ªå¯¹è¯åˆ°: {output_file}")
                print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶æ€»è®°å½•æ•°: {len(final_data)}")
                print(f"ğŸ’¡ ç»­ä¼ å‘½ä»¤: python generate_complete_dataset.py --start_from {next_start_from}")
            else:
                logger.info("æ²¡æœ‰ä¸´æ—¶æ–‡ä»¶æ•°æ®éœ€è¦åˆå¹¶")
                print("æ²¡æœ‰éœ€è¦ä¿å­˜çš„ä¸´æ—¶æ•°æ®")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            await self._cleanup_temp_files(temp_files)
            
            # æ–­å¼€MCPè¿æ¥
            try:
                await self.mcp_client.disconnect()
                logger.info("å·²æ–­å¼€MCPæœåŠ¡å™¨è¿æ¥")
            except Exception as e:
                logger.warning(f"æ–­å¼€MCPè¿æ¥æ—¶å‡ºé”™: {e}")
                
        except Exception as e:
            logger.error(f"å¤„ç†ç¨‹åºä¸­æ–­æ—¶å‡ºé”™: {e}")

    async def _cleanup_temp_files(self, temp_files: List[str]):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            cleanup_count = 0
            for temp_file in temp_files:
                if os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                        cleanup_count += 1
                        logger.debug(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {temp_file} å¤±è´¥: {e}")
            
            if cleanup_count > 0:
                logger.info(f"ğŸ§¹ å·²æ¸…ç† {cleanup_count} ä¸ªä¸´æ—¶æ–‡ä»¶")
                
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    async def _save_results_incrementally(self, new_data: List[Dict], output_file: str):
        """ä»¥å¢é‡æ–¹å¼ä¿å­˜ç»“æœï¼Œä¸è¦†ç›–ç°æœ‰æ•°æ®"""
        try:
            if not new_data:
                logger.info("æ²¡æœ‰æ–°æ•°æ®éœ€è¦ä¿å­˜")
                return
            
            # è¯»å–ç°æœ‰è¾“å‡ºæ–‡ä»¶çš„æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            existing_data = []
            if os.path.exists(output_file):
                try:
                    existing_data = self.utils.load_json_file(output_file)
                    if existing_data:
                        logger.info(f"å·²è¯»å–ç°æœ‰è¾“å‡ºæ–‡ä»¶: {output_file}, {len(existing_data)} æ¡è®°å½•")
                except Exception as e:
                    logger.warning(f"è¯»å–ç°æœ‰è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            
            # åˆå¹¶æ•°æ®ï¼šç°æœ‰æ•°æ® + æ–°æ•°æ®
            final_data = existing_data + new_data
            
            # ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶
            self.utils.write_json_file(final_data, output_file)
            logger.info(f"ğŸ’¾ å·²å°† {len(new_data)} ä¸ªæ–°è®°å½•è¿½åŠ åˆ°: {output_file}")
            logger.info(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶ç°æœ‰è®°å½•æ•°: {len(final_data)} (åŸæœ‰ {len(existing_data)} + æ–°å¢ {len(new_data)})")
            
        except Exception as e:
            logger.error(f"å¢é‡ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")

    async def complete_batch(self, question_data: List[Dict], batch_num: int) -> List[Dict]:
        """è¡¥å…¨ä¸€æ‰¹å¯¹è¯æ•°æ®ï¼Œä½¿ç”¨çœŸå®çš„function calling"""
        logger.info(f"å¼€å§‹è¡¥å…¨ç¬¬ {batch_num} æ‰¹æ•°æ®ï¼Œå…± {len(question_data)} ä¸ªå¯¹è¯")
        
        # è·å–MCPå·¥å…·schema
        tools_schema = await self._get_mcp_tools_schema()
        
        # æ£€æŸ¥å¹¶å‘é…ç½®
        enable_parallel = self.config.get('completion.enable_parallel_completion', True)
        max_concurrent = self.config.get('completion.max_concurrent_completions', 2)
        function_call_timeout = self.config.get('completion.function_call_timeout', 30)
        
        completed_conversations = []
        
        # é¢„å¤„ç†ï¼šè¿‡æ»¤å‡ºéœ€è¦è¡¥å…¨çš„å¯¹è¯
        items_to_complete = []
        for i, item in enumerate(question_data):
            conversations = item.get("conversations", [])
            if not conversations:
                logger.warning(f"ç¬¬ {i+1} ä¸ªå¯¹è¯ç¼ºå°‘conversationså­—æ®µ")
                continue
                
            # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæ•´
            def is_conversation_complete(conversations):
                """æ£€æŸ¥å¯¹è¯æ˜¯å¦çœŸæ­£å®Œæ•´"""
                message_types = [conv.get("from") for conv in conversations]
                
                # å¯¹äºfunction callingæ•°æ®é›†ï¼Œå¿…é¡»åŒ…å«gptçš„æœ€ç»ˆå›å¤
                return "gpt" in message_types
            
            if is_conversation_complete(conversations):
                logger.info(f"ç¬¬ {i+1} ä¸ªå¯¹è¯å·²å®Œæ•´ï¼Œè·³è¿‡")
                completed_conversations.append(item)
                continue
            
            items_to_complete.append((i, item))
        
        if not items_to_complete:
            logger.info("æ‰€æœ‰å¯¹è¯éƒ½å·²å®Œæ•´ï¼Œæ— éœ€è¡¥å…¨")
            return completed_conversations
        
        if enable_parallel and len(items_to_complete) > 1:
            # å¹¶è¡Œå¤„ç†æ¨¡å¼
            logger.info(f"ğŸš€ å¯ç”¨å¹¶è¡Œè¡¥å…¨ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
            
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def complete_single_conversation(index_item_pair):
                i, item = index_item_pair
                async with semaphore:
                    try:
                        conversations = item.get("conversations", [])
                        
                        # æå–ç”¨æˆ·é—®é¢˜ç”¨äºæ—¥å¿—
                        user_question = ""
                        for conv in conversations:
                            if conv.get("from") == "user":
                                user_question = conv.get("value", "")[:100] + "..." if len(conv.get("value", "")) > 100 else conv.get("value", "")
                                break
                        
                        logger.info(f"[å¹¶è¡Œ] æ­£åœ¨è¡¥å…¨å¯¹è¯ {i+1}/{len(question_data)}: {user_question}")
                        
                        # æ„å»ºæ¶ˆæ¯æ ¼å¼
                        messages = [{"role": "system", "content": self.system_prompt}]
                        for conv in conversations:
                            role = conv.get("from", "")
                            content = conv.get("value", "")
                            if role == "user":
                                messages.append({"role": "user", "content": content})
                            elif role == "system":
                                messages.append({"role": "system", "content": content})
                        
                        # ä½¿ç”¨timeoutæ§åˆ¶function calling
                        try:
                            result = await asyncio.wait_for(
                                self.api_client.generate_complete_conversation(
                                    messages=messages,
                                    tools_schema=tools_schema,
                                    mcp_client=self.mcp_client
                                ),
                                timeout=function_call_timeout
                            )
                        except asyncio.TimeoutError:
                            logger.error(f"[å¹¶è¡Œ] å¯¹è¯ {i+1} function callingè¶…æ—¶")
                            return None
                        
                        if result and result.get("new_messages"):
                            # ç»Ÿä¸€è§’è‰²åç§°ï¼ˆuser -> humanï¼‰
                            for conv in conversations:
                                if conv.get("from") == "user":
                                    conv["from"] = "human"
                            
                            # æ·»åŠ æ–°æ¶ˆæ¯
                            conversations.extend(result["new_messages"])
                            
                            completed_conversation = {
                                "conversations": conversations,
                                "tools": result.get("tools_used_json", "[]")
                            }
                            
                            logger.info(f"[å¹¶è¡Œ] æˆåŠŸè¡¥å…¨å¯¹è¯ {i+1}/{len(question_data)}")
                            return completed_conversation
                        else:
                            logger.error(f"[å¹¶è¡Œ] å¯¹è¯ {i+1} function callingå¤±è´¥")
                            return None
                            
                    except Exception as e:
                        logger.error(f"[å¹¶è¡Œ] è¡¥å…¨å¯¹è¯ {i+1} æ—¶å‡ºé”™: {e}")
                        return None
            
            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            tasks = [complete_single_conversation(item_pair) for item_pair in items_to_complete]
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"[å¹¶è¡Œ] ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {result}")
                    continue
                if result:
                    completed_conversations.append(result)
                    
        else:
            # ä¸²è¡Œå¤„ç†æ¨¡å¼
            logger.info("ğŸ“ ä½¿ç”¨ä¸²è¡Œè¡¥å…¨æ¨¡å¼")
            
            for i, item in items_to_complete:
                try:
                    conversations = item.get("conversations", [])
                    
                    # æå–ç”¨æˆ·é—®é¢˜ç”¨äºæ—¥å¿—
                    user_question = ""
                    for conv in conversations:
                        if conv.get("from") == "user":
                            user_question = conv.get("value", "")[:100] + "..." if len(conv.get("value", "")) > 100 else conv.get("value", "")
                            break
                    
                    logger.info(f"æ­£åœ¨è¡¥å…¨å¯¹è¯ {i+1}/{len(question_data)}: {user_question}")
                    
                    # æ„å»ºç”¨äºfunction callingçš„æ¶ˆæ¯æ ¼å¼
                    messages = [{"role": "system", "content": self.system_prompt}]
                    for conv in conversations:
                        role = conv.get("from", "")
                        content = conv.get("value", "")
                        if role == "user":
                            messages.append({"role": "user", "content": content})
                        elif role == "system":
                            messages.append({"role": "system", "content": content})
                        elif role == "assistant":
                            messages.append({"role": "assistant", "content": content})


                    # ä½¿ç”¨çœŸå®çš„function callingç”Ÿæˆå®Œæ•´å¯¹è¯
                    result = await self.api_client.generate_complete_conversation(
                        messages=messages,
                        tools_schema=tools_schema,
                        mcp_client=self.mcp_client
                    )
                    
                    if result and result.get("new_messages"):
                        # ç»Ÿä¸€è§’è‰²åç§°ï¼ˆuser -> humanï¼‰
                        for conv in conversations:
                            if conv.get("from") == "user":
                                conv["from"] = "human"
                        
                        # æ·»åŠ æ‰€æœ‰æ–°ç”Ÿæˆçš„æ¶ˆæ¯åˆ°å¯¹è¯ä¸­
                        conversations.extend(result["new_messages"])
                        # åˆ›å»ºå®Œæ•´çš„å¯¹è¯å¯¹è±¡ï¼ŒåŒ…å«toolså®šä¹‰
                        completed_conversation = {
                            "conversations": conversations,
                            "tools": result.get("tools_used_json", "[]")
                        }
                        
                        completed_conversations.append(completed_conversation)
                        logger.info(f"æˆåŠŸè¡¥å…¨å¯¹è¯ {i+1}/{len(question_data)}ï¼Œä½¿ç”¨äº†å·¥å…·è°ƒç”¨")
                    else:
                        logger.error(f"å¯¹è¯ {i+1} function callingå¤±è´¥ï¼Œè·³è¿‡")
                    
                    # é¿å…APIé™æµ
                    if (i + 1) % 5 == 0:
                        logger.info("æš‚åœ1ç§’é¿å…APIé™æµ")
                        await asyncio.sleep(1)
                        
                except Exception as e:
                    logger.error(f"è¡¥å…¨å¯¹è¯ {i+1} æ—¶å‡ºé”™: {e}")
                    continue
        
        processing_mode = "å¹¶è¡Œ" if enable_parallel and len(items_to_complete) > 1 else "ä¸²è¡Œ"
        success_rate = len(completed_conversations) / len(question_data) * 100 if question_data else 0
        logger.info(f"ç¬¬ {batch_num} æ‰¹è¡¥å…¨å®Œæˆ: {len(completed_conversations)}/{len(question_data)} (æˆåŠŸç‡: {success_rate:.1f}%, {processing_mode}å¤„ç†)")
        return completed_conversations

    async def generate_complete_dataset(self, question_file: str, output_file: str = "function_calling_dataset_completed.json", batch_size: int = 1, start_from: int = 0):
        """ç”Ÿæˆå®Œæ•´çš„æ•°æ®é›†"""
        logger.info(f"å¼€å§‹ç”Ÿæˆå®Œæ•´æ•°æ®é›†ï¼Œè¾“å…¥æ–‡ä»¶: {question_file}")
        
        # è¯»å–é—®é¢˜æ•°æ®
        question_data = self.utils.load_json_file(question_file)
        if not question_data:
            logger.error(f"æ— æ³•è¯»å–é—®é¢˜æ–‡ä»¶: {question_file}")
            return
        
        original_length = len(question_data)
        
        # å¦‚æœæŒ‡å®šäº†start_fromï¼Œåˆ™ä»è¯¥ä½ç½®å¼€å§‹å¤„ç†
        if start_from > 0:
            if start_from >= len(question_data):
                logger.error(f"èµ·å§‹ä½ç½® {start_from} è¶…å‡ºæ•°æ®èŒƒå›´ {len(question_data)}")
                return
            question_data = question_data[start_from:]
            logger.info(f"ä»ä½ç½® {start_from} å¼€å§‹å¤„ç†ï¼Œå‰©ä½™ {len(question_data)}/{original_length} ä¸ªå¯¹è¯")
        else:
            logger.info(f"æˆåŠŸè¯»å– {len(question_data)} ä¸ªå¯¹è¯ï¼Œå¼€å§‹è¡¥å…¨")
        
        # è¿æ¥MCPå®¢æˆ·ç«¯
        try:
            await self.mcp_client.connect()
            logger.info("æˆåŠŸè¿æ¥åˆ°MCPæœåŠ¡å™¨")
        except Exception as e:
            logger.error(f"è¿æ¥MCPæœåŠ¡å™¨å¤±è´¥: {e}")
            logger.info("å°†ä½¿ç”¨é»˜è®¤å·¥å…·å®šä¹‰ç»§ç»­æ‰§è¡Œ")
        
        all_completed = []
        total_batches = (len(question_data) + batch_size - 1) // batch_size
        
        logger.info(f"æ€»å…±éœ€è¦å¤„ç† {total_batches} æ‰¹æ•°æ®ï¼Œæ¯æ‰¹ {batch_size} ä¸ªå¯¹è¯")
        
        # ç”¨äºå­˜å‚¨ä¸´æ—¶æ–‡ä»¶åï¼Œä¾¿äºæ¸…ç†
        temp_files = []
        
        try:
            for batch_num in range(1, total_batches + 1):
                # æ£€æŸ¥æ˜¯å¦æ”¶åˆ°ä¸­æ–­ä¿¡å·
                if hasattr(self, '_interrupted') and self._interrupted:
                    logger.warning("æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢å¤„ç†...")
                    raise KeyboardInterrupt("ç¨‹åºè¢«ä¿¡å·ä¸­æ–­")
                
                start_idx = (batch_num - 1) * batch_size
                end_idx = min(start_idx + batch_size, len(question_data))
                batch_data = question_data[start_idx:end_idx]
                
                try:
                    logger.info(f"å¼€å§‹å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹æ•°æ®")
                    completed_batch = await self.complete_batch(batch_data, batch_num)
                    all_completed.extend(completed_batch)
                    
                    # ä¿å­˜ä¸­é—´ç»“æœ - ä½¿ç”¨åŒ…å«start_fromçš„æ–‡ä»¶å
                    actual_batch_start = start_from + start_idx
                    temp_file = f"temp_complete_from_{start_from}_batch_{batch_num}_start_{actual_batch_start}.json"
                    temp_files.append(temp_file)
                    self.utils.write_json_file(completed_batch, temp_file)
                    logger.info(f"ä¸­é—´ç»“æœå·²ä¿å­˜åˆ°: {temp_file}")
                    
                    progress = len(all_completed) / len(question_data) * 100
                    logger.info(f"æ€»ä½“è¿›åº¦: {len(all_completed)}/{len(question_data)} ({progress:.1f}%)")
                    
                    # é¿å…APIé™æµ
                    if batch_num < total_batches:
                        logger.info("æ‰¹æ¬¡é—´æš‚åœ3ç§’")
                        await asyncio.sleep(3)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {batch_num} æ‰¹æ•°æ®æ—¶å‡ºé”™: {e}")
                    continue
        
        except KeyboardInterrupt:
            logger.warning("æ£€æµ‹åˆ°ç¨‹åºä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å·²å®Œæˆçš„æ•°æ®...")
            await self._handle_interruption(all_completed, temp_files, output_file, start_from)
            raise
        except Exception as e:
            logger.error(f"ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            await self._handle_interruption(all_completed, temp_files, output_file, start_from)
            raise
        
        # æ–­å¼€MCPè¿æ¥
        try:
            await self.mcp_client.disconnect()
            logger.info("å·²æ–­å¼€MCPæœåŠ¡å™¨è¿æ¥")
        except Exception as e:
            logger.warning(f"æ–­å¼€MCPè¿æ¥æ—¶å‡ºé”™: {e}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ - ä½¿ç”¨å¢é‡æ–¹å¼ï¼Œä¸è¦†ç›–ç°æœ‰æ•°æ®
        await self._save_results_incrementally(all_completed, output_file)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        await self._cleanup_temp_files(temp_files)
        
        final_success_rate = len(all_completed) / len(question_data) * 100 if question_data else 0
        logger.info(f"å®Œæ•´æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
        logger.info(f"æ€»è®¡å®Œæˆ: {len(all_completed)}/{len(question_data)} ä¸ªå¯¹è¯ (æˆåŠŸç‡: {final_success_rate:.1f}%)")
        logger.info(f"æœ€ç»ˆç»“æœä¿å­˜è‡³: {output_file}")
        
        return all_completed


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="ç”Ÿæˆå®Œæ•´çš„Merlin Chain function callingæ•°æ®é›†")
    parser.add_argument("--question_file", type=str, help="é—®é¢˜æ•°æ®æ–‡ä»¶ (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)")
    parser.add_argument("--output_file", type=str, help="è¾“å‡ºæ–‡ä»¶å (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)")
    parser.add_argument("--batch_size", type=int, help="æ¯æ‰¹å¤„ç†çš„å¯¹è¯æ•°é‡ (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)")
    parser.add_argument("--start_from", type=int, default=None, help="ä»æŒ‡å®šä½ç½®å¼€å§‹å¤„ç† (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)")
    parser.add_argument("--config", type=str, default="config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.json)")
    parser.add_argument("--api_key", type=str, help="DeepSeek API Key (å¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®)")
    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åŠ è½½é…ç½®
    logger.info(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    config = ConfigManager(args.config)
    
    # å¦‚æœå‘½ä»¤è¡Œæä¾›äº†API Keyï¼Œåˆ™è¦†ç›–é…ç½®æ–‡ä»¶
    if args.api_key:
        config.set_api_key(args.api_key)
        logger.info("âœ… ä½¿ç”¨å‘½ä»¤è¡Œæä¾›çš„API Key")
    
    # éªŒè¯é…ç½®
    if not config.validate_config():
        logger.error("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        return
    
    # ä»é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°è·å–è®¾ç½®
    question_file = args.question_file or config.get('completion.default_question_file', "function_calling_dataset_smart.json")
    output_file = args.output_file or config.get('completion.default_output_file', "function_calling_dataset_completed.json")
    batch_size = args.batch_size or config.get('completion.default_batch_size', 1)
    start_from = args.start_from if args.start_from is not None else config.get('completion.default_start_from', 0)
    system_prompt_file = config.get('completion.system_prompt_file', 'prompt-2.txt')
    
    api_key = config.get_api_key()
    logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆå®Œæ•´çš„function callingæ•°æ®é›†...")
    logger.info(f"   API Keyå‰ç¼€: {api_key[:8]}...")
    logger.info(f"ğŸ“– è¾“å…¥æ–‡ä»¶: {question_file}")
    logger.info(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
    logger.info(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    if start_from > 0:
        logger.info(f"   èµ·å§‹ä½ç½®: {start_from}")
    logger.info(f"ğŸ“ ç³»ç»Ÿæç¤ºè¯: {system_prompt_file}")
    logger.info(f"ğŸ”§ MCPå·¥å…·: è‡ªåŠ¨æ³¨å†Œæ‰€æœ‰å¯ç”¨å·¥å…·")
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    generator = None
    
    def signal_handler(signum, frame):
        logger.warning(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
        if generator:
            # è¿™é‡Œåªèƒ½è®¾ç½®æ ‡è®°ï¼Œå®é™…å¤„ç†åœ¨å¼‚æ­¥ä»£ç ä¸­è¿›è¡Œ
            generator._interrupted = True
        print("\nâš ï¸  ç¨‹åºæ­£åœ¨å®‰å…¨é€€å‡ºï¼Œè¯·ç¨å€™...")
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·
    
    try:
        generator = CompleteDatasetGenerator(config)
        generator._interrupted = False  # æ·»åŠ ä¸­æ–­æ ‡è®°
        
        # ç”Ÿæˆå®Œæ•´æ•°æ®é›†
        completed_data = await generator.generate_complete_dataset(
            question_file=question_file,
            output_file=output_file,
            batch_size=batch_size,
            start_from=start_from
        )
        
        if completed_data:
            logger.info(f"âœ… å®Œæ•´æ•°æ®é›†ç”ŸæˆæˆåŠŸï¼Œå…± {len(completed_data)} ä¸ªå¯¹è¯")
            print(f"âœ… å®Œæ•´æ•°æ®é›†ç”ŸæˆæˆåŠŸï¼Œå…± {len(completed_data)} ä¸ªå¯¹è¯")
        else:
            logger.error("âŒ å®Œæ•´æ•°æ®é›†ç”Ÿæˆå¤±è´¥")
            print("âŒ å®Œæ•´æ•°æ®é›†ç”Ÿæˆå¤±è´¥")
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
        print("ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
    except ValueError as e:
        logger.error(f"âŒ é…ç½®é”™è¯¯: {e}")
        print(f"âŒ é…ç½®é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥config.jsonæ–‡ä»¶æˆ–è®¾ç½®ç¯å¢ƒå˜é‡DEEPSEEK_API_KEY")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    asyncio.run(main())