#!/usr/bin/env python3
"""
é€šè¿‡deepseek APIç”ŸæˆMerlin Chain function callingé—®é¢˜æ•°æ®é›†ï¼ˆæ™ºèƒ½å»é‡ç‰ˆæœ¬ï¼‰
åŸºäºå»é‡ç®¡ç†å™¨ï¼Œé¿å…é‡å¤ç”Ÿæˆï¼Œä¿æŒå·¥å…·åˆ†å¸ƒå‡è¡¡
"""
import asyncio
import argparse
import signal
import atexit
from typing import List, Dict
from deepseek_api_client import DeepSeekAPIClient
from dataset_utils import DatasetUtils
from dedup_manager import DatasetDedupManager
from config_manager import ConfigManager


class SmartQuestionDatasetGenerator:
    """æ™ºèƒ½é—®é¢˜æ•°æ®é›†ç”Ÿæˆå™¨ï¼Œå…·å¤‡å»é‡å’Œè¿›åº¦è·Ÿè¸ªåŠŸèƒ½"""
    
    def __init__(self, config_manager: ConfigManager):
        self.config = config_manager
        api_key = self.config.get_api_key()
        if not api_key:
            raise ValueError("API Keyæœªé…ç½®ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡")
        
        # è·å–æ€»å¯¹è¯æ•°é…ç½®
        total_conversations = self.config.get('generation.default_total_conversations', 6000)
        
        self.api_client = DeepSeekAPIClient(api_key)
        self.utils = DatasetUtils()
        self.dedup_manager = DatasetDedupManager(total_conversations=total_conversations)
        self.output_file = self.config.get('generation.default_output_file', "function_calling_dataset_smart.json")
        self.current_conversations = []
        self._setup_cleanup_handlers()
    
    def _setup_cleanup_handlers(self):
        """è®¾ç½®æ¸…ç†å¤„ç†å™¨ï¼ˆç±»ä¼¼Goçš„deferï¼‰"""
        # æ³¨å†Œç¨‹åºæ­£å¸¸é€€å‡ºæ—¶çš„æ¸…ç†
        atexit.register(self._emergency_cleanup)
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨å¤„ç†Ctrl+Cç­‰ä¸­æ–­
        signal.signal(signal.SIGINT, self._signal_cleanup_handler)
        signal.signal(signal.SIGTERM, self._signal_cleanup_handler)
        
        print("ğŸ›¡ï¸ å·²è®¾ç½®åº”æ€¥æ¸…ç†æœºåˆ¶ï¼ˆæ”¯æŒCtrl+Cå®‰å…¨é€€å‡ºï¼‰")
    
    def _signal_cleanup_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨ï¼ˆå¤„ç†Ctrl+Cç­‰ä¸­æ–­ä¿¡å·ï¼‰"""
        print(f"\nâš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å· {signum}ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
        self._emergency_cleanup()
        print("âœ… æ•°æ®å·²å®‰å…¨ä¿å­˜ï¼Œç¨‹åºé€€å‡º")
        exit(0)
    
    def _emergency_cleanup(self):
        """åº”æ€¥æ¸…ç†å‡½æ•°ï¼ˆç±»ä¼¼deferï¼‰"""
        try:
            print("\nğŸš¨ æ‰§è¡Œåº”æ€¥æ•°æ®ä¿æŠ¤...")
            
            # åˆå¹¶æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶å¹¶ä¿å­˜
            if hasattr(self, 'current_conversations'):
                final_conversations = self._merge_all_temp_files(self.current_conversations)
                
                # åªä¿å­˜åˆ°ä¸»è¾“å‡ºæ–‡ä»¶ï¼ˆå–æ¶ˆåº”æ€¥å¤‡ä»½ï¼‰
                self.utils.write_json_file(final_conversations, self.output_file)
                
                print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {self.output_file}")
                print(f"ğŸ“Š å…±ä¿å­˜ {len(final_conversations)} ä¸ªå¯¹è¯")
                
        except Exception as e:
            print(f"âŒ åº”æ€¥æ¸…ç†å¤±è´¥: {e}")
    
    def _defer_cleanup(self):
        """æ‰‹åŠ¨è§¦å‘æ¸…ç†ï¼ˆæ¨¡æ‹Ÿdeferè°ƒç”¨ï¼‰"""
        self._emergency_cleanup()
        

    async def generate_batch(self, base_system_prompt: str, batch_num: int, batch_size: int = 10) -> List[Dict]:
        """ç”Ÿæˆä¸€æ‰¹å¯¹è¯æ•°æ®ï¼ˆé›†æˆå»é‡é€»è¾‘ï¼‰"""
        print(f"æ­£åœ¨ç”Ÿæˆç¬¬ {batch_num} æ‰¹æ•°æ®...")
        
        # è·å–æœ¬æ‰¹æ¬¡çš„å·¥å…·åˆ†é…è®¡åˆ’
        tool_allocation = self.dedup_manager._get_priority_tools(batch_size)
        if not tool_allocation:
            print(f"æ‰€æœ‰å·¥å…·å·²å®Œæˆç›®æ ‡ï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_num}")
            return []
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶è¡Œç”Ÿæˆ
        enable_parallel = self.config.get('generation.enable_parallel_generation', True)
        max_concurrent = self.config.get('generation.max_concurrent_tools', 3)
        parallel_delay = self.config.get('generation.parallel_batch_delay', 0.5)
        
        batch_conversations = []
        
        if enable_parallel and len(tool_allocation) > 1:
            # å¹¶è¡Œå¤„ç†æ¨¡å¼
            print(f"  ğŸš€ å¯ç”¨å¹¶è¡Œå¤„ç†ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
            
            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_tool_with_semaphore(tool_name: str, target_count: int):
                async with semaphore:
                    print(f"  ğŸ¯ [å¹¶è¡Œ] æ­£åœ¨ç”Ÿæˆ {tool_name} çš„ {target_count} ä¸ªé—®é¢˜...")
                    try:
                        result = await self._generate_for_specific_tool(
                            base_system_prompt, tool_name, target_count, batch_num
                        )
                        if parallel_delay > 0:
                            await asyncio.sleep(parallel_delay)
                        return tool_name, result
                    except Exception as e:
                        print(f"  âŒ [å¹¶è¡Œ] ç”Ÿæˆ {tool_name} å¤±è´¥: {e}")
                        return tool_name, []
            
            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            tasks = []
            for tool_name, target_count in tool_allocation:
                if target_count > 0:
                    task = process_tool_with_semaphore(tool_name, target_count)
                    tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    print(f"  âŒ [å¹¶è¡Œ] ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {result}")
                    continue
                
                tool_name, tool_conversations = result
                for conv_data in tool_conversations:
                    conversations_list = conv_data.get("conversations", [])
                    user_question = self._extract_user_question(conversations_list)
                    
                    if user_question and not self.dedup_manager.check_duplicate(user_question):
                        user_role = self._infer_user_role(user_question)
                        language_style = self._infer_language_style(user_question)
                        
                        self.dedup_manager.record_generated(
                            conversations_list, tool_name, user_role, language_style
                        )
                        
                        batch_conversations.append(conv_data)
                        print(f"    âœ… [å¹¶è¡Œ] è®°å½•: {tool_name} - {user_question[:50]}...")
        else:
            # ä¸²è¡Œå¤„ç†æ¨¡å¼
            print(f"  ğŸ“ ä½¿ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼")
            for tool_name, target_count in tool_allocation:
                if target_count <= 0:
                    continue
                    
                print(f"  ğŸ¯ æ­£åœ¨ç”Ÿæˆ {tool_name} çš„ {target_count} ä¸ªé—®é¢˜...")
                
                # ä¸ºç‰¹å®šå·¥å…·ç”Ÿæˆå¯¹è¯
                tool_conversations = await self._generate_for_specific_tool(
                    base_system_prompt, tool_name, target_count, batch_num
                )
                
                # ç›´æ¥æ›´æ–°çŠ¶æ€ï¼ˆä¸éœ€è¦æ¨æ–­ï¼‰
                for conv_data in tool_conversations:
                    conversations_list = conv_data.get("conversations", [])
                    user_question = self._extract_user_question(conversations_list)
                    
                    if user_question and not self.dedup_manager.check_duplicate(user_question):
                        # ç›´æ¥ä½¿ç”¨å·²çŸ¥çš„å·¥å…·åæ›´æ–°çŠ¶æ€
                        user_role = self._infer_user_role(user_question)
                        language_style = self._infer_language_style(user_question)
                        
                        self.dedup_manager.record_generated(
                            conversations_list, tool_name, user_role, language_style
                        )
                        
                        batch_conversations.append(conv_data)
                        print(f"    âœ… è®°å½•: {tool_name} - {user_question[:50]}...")
        
        processing_mode = "å¹¶è¡Œ" if enable_parallel and len(tool_allocation) > 1 else "ä¸²è¡Œ"
        print(f"ç¬¬ {batch_num} æ‰¹æˆåŠŸç”Ÿæˆ {len(batch_conversations)} ä¸ªå¯¹è¯ ({processing_mode}å¤„ç†)")
        return batch_conversations
    
    def _extract_user_question(self, conversations_list: List[Dict]) -> str:
        """æå–ç”¨æˆ·é—®é¢˜"""
        for conv in conversations_list:
            if conv.get("from") == "user":
                return conv.get("value", "")
        return ""
    
    async def _generate_for_specific_tool(self, base_system_prompt: str, tool_name: str, 
                                        count: int, batch_num: int) -> List[Dict]:
        """ä¸ºç‰¹å®šå·¥å…·ç”Ÿæˆå¯¹è¯"""
        
        # å·¥å…·ç‰¹å®šçš„ç”Ÿæˆæç¤º
        tool_prompts = {
            "get_address_details_by_address": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢é’±åŒ…åœ°å€è¯¦æƒ…ã€ä½™é¢ã€åŸºæœ¬ä¿¡æ¯çš„é—®é¢˜",
            "get_token_info_by_address": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢ä»£å¸ä¿¡æ¯ã€ä»£å¸è¯¦æƒ…çš„é—®é¢˜", 
            "list_address_latest_txs": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢åœ°å€æœ€æ–°äº¤æ˜“è®°å½•ã€äº¤æ˜“å†å²çš„é—®é¢˜",
            "get_tx_by_hash": f"ç”Ÿæˆ{count}ä¸ªå…³äºé€šè¿‡äº¤æ˜“å“ˆå¸ŒæŸ¥è¯¢äº¤æ˜“è¯¦æƒ…çš„é—®é¢˜",
            "search_chain_data": f"ç”Ÿæˆ{count}ä¸ªå…³äºæœç´¢é“¾ä¸Šæ•°æ®ã€æŸ¥æ‰¾ä»£å¸æˆ–åœ°å€çš„é—®é¢˜",
            "query_asset_value_by_address": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢åœ°å€æ€»èµ„äº§ä»·å€¼çš„é—®é¢˜",
            "query_token_holding_by_address": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢åœ°å€æŒä»“åˆ†æã€ä»£å¸åˆ†å¸ƒçš„é—®é¢˜",
            "get_block_by_number": f"ç”Ÿæˆ{count}ä¸ªå…³äºé€šè¿‡åŒºå—å·æŸ¥è¯¢åŒºå—è¯¦æƒ…çš„é—®é¢˜",
            "list_latest_blocks": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢æœ€æ–°åŒºå—åˆ—è¡¨çš„é—®é¢˜",
            "get_token_priceChange_by_address": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢ä»£å¸ä»·æ ¼å˜åŒ–ã€æ¶¨è·Œå¹…çš„é—®é¢˜",
            "list_address_latest_token_transfers": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢ä»£å¸è½¬è´¦è®°å½•çš„é—®é¢˜",
            "get_holders_by_address": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢ä»£å¸æŒæœ‰è€…æ’è¡Œçš„é—®é¢˜",
            "batch_get_tx_by_hashes": f"ç”Ÿæˆ{count}ä¸ªå…³äºæ‰¹é‡æŸ¥è¯¢å¤šä¸ªäº¤æ˜“å“ˆå¸Œçš„é—®é¢˜",
            "list_block_txs": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢åŒºå—å†…äº¤æ˜“åˆ—è¡¨çš„é—®é¢˜",
            "get_native_price_info_by_address": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢BTCåŸç”Ÿä»£å¸ä»·æ ¼çš„é—®é¢˜",
            "get_token_onChain_data_by_address": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢ä»£å¸é“¾ä¸Šæ•°æ®ã€äº¤æ˜“é‡çš„é—®é¢˜",
            "list_recent_txs_num_by_address": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢åœ°å€äº¤æ˜“æ•°é‡ç»Ÿè®¡çš„é—®é¢˜",
            "get_block_by_hash": f"ç”Ÿæˆ{count}ä¸ªå…³äºé€šè¿‡åŒºå—å“ˆå¸ŒæŸ¥è¯¢åŒºå—çš„é—®é¢˜",
            "list_latest_txs": f"ç”Ÿæˆ{count}ä¸ªå…³äºæŸ¥è¯¢æœ€æ–°äº¤æ˜“åˆ—è¡¨çš„é—®é¢˜",
        }
        
        user_prompt = tool_prompts.get(tool_name, f"ç”Ÿæˆ{count}ä¸ªå…³äº{tool_name}çš„é—®é¢˜")
        user_prompt += "ï¼Œæ³¨æ„ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯é‡Œé¢ç»™çš„ç¤ºä¾‹åœ°å€å’Œå“ˆå¸Œå‚æ•°ï¼Œå¹¶ä¸”æŒ‰ç…§ç¤ºä¾‹æ ¼å¼è¿”å›ã€‚"
        
        # è°ƒç”¨APIç”Ÿæˆ
        response = self.api_client.call_api(base_system_prompt, user_prompt)
        if not response:
            return []
        
        # è§£æå¹¶è¿”å›å¯¹è¯
        conversations = self.utils.extract_json_from_response(response)
        return conversations if conversations else []

    
    def _infer_user_role(self, question: str) -> str:
        """æ¨æ–­ç”¨æˆ·è§’è‰²"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ["æ€ä¹ˆçœ‹", "æˆåŠŸæ²¡", "æœ‰æ²¡æœ‰", "æ˜¯ä»€ä¹ˆ", "ä¸æ‡‚"]):
            return "åŒºå—é“¾å°ç™½"
        
        if any(word in question_lower for word in ["æ‰¹é‡", "api", "æ¥å£", "è°ƒè¯•", "erc-20"]):
            return "åŒºå—é“¾å¼€å‘è€…"
        
        if any(word in question_lower for word in ["æŒä»“", "èµ„äº§", "ä»·æ ¼", "æŠ•èµ„", "æ”¶ç›Š"]):
            return "åŒºå—é“¾æŠ•èµ„è€…"
        
        if any(word in question_lower for word in ["åˆ†æ", "å¯¹æ¯”", "æŒ–æ˜", "å¤æ‚"]):
            return "åŒºå—é“¾ä¸“å®¶"
        
        return "åŒºå—é“¾å°ç™½"
    
    def _infer_language_style(self, question: str) -> str:
        """æ¨æ–­è¯­è¨€é£æ ¼"""
        if any(word in question for word in ["ERC-20", "0x", "hash", "address", "token"]):
            return "æŠ€æœ¯ç”¨è¯­"
        
        if any(word in question for word in ["address", "balance", "transaction"]) and \
           any('\u4e00' <= char <= '\u9fff' for char in question):
            return "ä¸­è‹±æ··åˆ"
        
        if "0x" in question and len([x for x in question.split() if x.startswith("0x") and len(x) < 42]):
            return "é”™è¯¯è¡¨è¾¾"
        
        return "å£è¯­åŒ–"

    def _merge_all_temp_files(self, current_conversations: List[Dict]) -> List[Dict]:
        """åˆå¹¶æ‰€æœ‰ç›¸å…³çš„ä¸´æ—¶æ–‡ä»¶"""
        import glob
        import os
        
        # æ‰¾åˆ°æ‰€æœ‰æ™ºèƒ½ç‰ˆæœ¬çš„ä¸´æ—¶æ–‡ä»¶
        temp_files = glob.glob("temp_smart_question_batch_*.json")
        
        all_conversations = []
        processed_files = []
        
        # è¯»å–æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        for temp_file in temp_files:
            try:
                temp_data = self.utils.load_json_file(temp_file)
                if temp_data:
                    all_conversations.extend(temp_data)
                    processed_files.append(temp_file)
            except Exception as e:
                print(f"    è­¦å‘Šï¼šè¯»å–ä¸´æ—¶æ–‡ä»¶ {temp_file} å¤±è´¥: {e}")
        
        # æ·»åŠ å½“å‰ä¼šè¯çš„å¯¹è¯
        all_conversations.extend(current_conversations)
        
        # å»é‡ï¼ˆåŸºäºconversationså†…å®¹ï¼‰
        unique_conversations = []
        seen_signatures = set()
        
        for conv_data in all_conversations:
            conversations_list = conv_data.get("conversations", [])
            if not conversations_list:
                continue
                
            # æå–ç”¨æˆ·é—®é¢˜ä½œä¸ºå”¯ä¸€æ ‡è¯†
            user_question = None
            for conv in conversations_list:
                if conv.get("from") == "user":
                    user_question = conv.get("value", "")
                    break
            
            if user_question:
                signature = self.dedup_manager.get_question_signature(user_question)
                if signature not in seen_signatures:
                    seen_signatures.add(signature)
                    unique_conversations.append(conv_data)
        
        print(f"ğŸ“ åˆå¹¶äº† {len(temp_files)} ä¸ªä¸´æ—¶æ–‡ä»¶")
        print(f"ğŸ“Š åˆå¹¶å‰æ€»æ•°: {len(all_conversations)}ï¼Œå»é‡å: {len(unique_conversations)}")
        
        # åˆå¹¶å®Œæˆååˆ é™¤æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        self._cleanup_temp_files(processed_files)
        
        return unique_conversations
    
    def _cleanup_temp_files(self, temp_files: List[str]):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆåˆå¹¶å®Œæˆååˆ é™¤æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼‰"""
        import os
        
        # åˆ é™¤æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        for file_path in temp_files:
            try:
                os.remove(file_path)
                print(f"    ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                print(f"    âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    async def generate_dataset(self, prompt_file: str, total_conversations: int = 6000, batch_size: int = 50, output_file: str = "function_calling_dataset_smart.json"):
        """ç”Ÿæˆå®Œæ•´çš„é—®é¢˜æ•°æ®é›†ï¼ˆæ™ºèƒ½å»é‡ç‰ˆæœ¬ï¼‰"""
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶ï¼ˆç”¨äºåº”æ€¥æ¸…ç†ï¼‰
        self.output_file = output_file
        
        # è¯»å–ç³»ç»Ÿæç¤ºè¯
        system_prompt = self.utils.read_file(prompt_file)
        if not system_prompt:
            print("æ— æ³•è¯»å–promptæ–‡ä»¶")
            return
        
        all_conversations = []
        total_batches = (total_conversations + batch_size - 1) // batch_size
        
        print(f"ğŸ§  å¼€å§‹æ™ºèƒ½ç”Ÿæˆé—®é¢˜æ•°æ®é›†ï¼Œç›®æ ‡: {total_conversations} ä¸ªå¯¹è¯ï¼Œåˆ† {total_batches} æ‰¹ç”Ÿæˆ")
        print(f"âœ¨ å¯ç”¨å»é‡æ£€æŸ¥å’Œè¿›åº¦è·Ÿè¸ª")
        
        # æ˜¾ç¤ºå½“å‰è¿›åº¦
        stats = self.dedup_manager.get_statistics()
        print(f"ğŸ“Š å½“å‰è¿›åº¦: {stats['æ€»è¿›åº¦']} ({stats['å®Œæˆç‡']})")
        
        for batch_num in range(1, total_batches + 1):
            try:
                # æ˜¾ç¤ºæœ¬æ‰¹æ¬¡ç›®æ ‡
                batch_guidance = self.dedup_manager.get_next_batch_prompt(batch_size)
                priority_info = [line for line in batch_guidance.split('\n') if 'åˆ†é…' in line][:3]
                if priority_info:
                    print(f"ğŸ¯ ç¬¬{batch_num}æ‰¹æ¬¡é‡ç‚¹: {'; '.join(priority_info)}")
                
                batch_conversations = await self.generate_batch(system_prompt, batch_num, batch_size)
                all_conversations.extend(batch_conversations)
                
                # æ›´æ–°å½“å‰å¯¹è¯çŠ¶æ€ï¼ˆç”¨äºåº”æ€¥æ¸…ç†ï¼‰
                self.current_conversations = all_conversations
                
                # ä¿å­˜ä¸­é—´ç»“æœï¼ˆæ·»åŠ æ—¶é—´æˆ³é¿å…è¦†ç›–ï¼‰
                import time
                timestamp = int(time.time())
                self.utils.write_json_file(batch_conversations, f"temp_smart_question_batch_{batch_num}_{timestamp}.json")
                
                # æ›´æ–°è¿›åº¦æ˜¾ç¤º
                current_stats = self.dedup_manager.get_statistics()
                print(f"ğŸ“ˆ ç´¯è®¡ç”Ÿæˆ: {len(all_conversations)} ä¸ªå¯¹è¯ (ç›®æ ‡è¿›åº¦: {current_stats['å®Œæˆç‡']})")
                
                # æ¯10æ‰¹æ¬¡æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
                if batch_num % 10 == 0:
                    print(f"\nğŸ“Š ç¬¬{batch_num}æ‰¹æ¬¡å®Œæˆï¼Œå½“å‰ç»Ÿè®¡:")
                    for role, count in current_stats['è§’è‰²åˆ†å¸ƒ'].items():
                        print(f"    {role}: {count}ä¸ª")
                    print()
                
                # é¿å…APIé™æµ
                if batch_num < total_batches:
                    await asyncio.sleep(2)
                
            except Exception as e:
                print(f"âŒ ç”Ÿæˆç¬¬ {batch_num} æ‰¹æ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        # ä¿å­˜æœ€ç»ˆç»“æœï¼ˆåˆå¹¶æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼‰
        final_conversations = self._merge_all_temp_files(all_conversations)
        self.utils.write_json_file(final_conversations, output_file)
        
        # æ­£å¸¸å®Œæˆï¼Œæ‰§è¡Œdeferæ¸…ç†
        try:
            # å–æ¶ˆæ³¨å†Œatexitï¼Œé¿å…é‡å¤æ‰§è¡Œ
            atexit.unregister(self._emergency_cleanup)
        except:
            pass
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_stats = self.dedup_manager.get_statistics()
        print(f"\nğŸ‰ æ™ºèƒ½é—®é¢˜æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ˆ æ€»è®¡ç”Ÿæˆ: {len(final_conversations)} ä¸ªå¯¹è¯")
        print(f"ğŸ’¾ ä¿å­˜è‡³: {output_file}")
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   å®Œæˆç‡: {final_stats['å®Œæˆç‡']}")
        print(f"   å·²ç”¨é—®é¢˜æ¨¡å¼: {final_stats['å·²ç”¨é—®é¢˜æ¨¡å¼']}ä¸ª")
        print(f"   å·²ç”¨åœ°å€: {final_stats['å·²ç”¨åœ°å€']}ä¸ª")
        print(f"   å·²ç”¨äº¤æ˜“å“ˆå¸Œ: {final_stats['å·²ç”¨äº¤æ˜“å“ˆå¸Œ']}ä¸ª")
        
        print(f"\nğŸ”§ å·¥å…·åˆ†å¸ƒ:")
        for tool, progress in list(final_stats['å·¥å…·è¿›åº¦'].items())[:5]:
            print(f"   {tool}: {progress}")
        
        print(f"\nğŸ‘¥ ç”¨æˆ·è§’è‰²åˆ†å¸ƒ:")
        for role, count in final_stats['è§’è‰²åˆ†å¸ƒ'].items():
            print(f"   {role}: {count}ä¸ª")
        
        print(f"ğŸ§¹ æ­£å¸¸å®Œæˆï¼Œä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        
        return final_conversations


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="ç”ŸæˆMerlin Chain function callingé—®é¢˜æ•°æ®é›†ï¼ˆæ™ºèƒ½å»é‡ç‰ˆæœ¬ï¼‰")
    parser.add_argument("--total_conversations", type=int, help="ç›®æ ‡å¯¹è¯æ•°é‡ (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)")
    parser.add_argument("--batch_size", type=int, help="æ¯æ‰¹ç”Ÿæˆçš„å¯¹è¯æ•°é‡ (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)")
    parser.add_argument("--output_file", type=str, help="è¾“å‡ºæ–‡ä»¶å (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)")
    parser.add_argument("--prompt_file", type=str, help="æç¤ºè¯æ–‡ä»¶ (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)")
    parser.add_argument("--config", type=str, default="config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.json)")
    parser.add_argument("--api_key", type=str, help="DeepSeek API Key (å¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®)")
    parser.add_argument("--reset", action="store_true", help="é‡ç½®ç”ŸæˆçŠ¶æ€ï¼Œä»å¤´å¼€å§‹")
    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    config = ConfigManager(args.config)
    
    # å¦‚æœå‘½ä»¤è¡Œæä¾›äº†API Keyï¼Œåˆ™è¦†ç›–é…ç½®æ–‡ä»¶
    if args.api_key:
        config.set_api_key(args.api_key)
        print("âœ… ä½¿ç”¨å‘½ä»¤è¡Œæä¾›çš„API Key")
    
    # éªŒè¯é…ç½®
    if not config.validate_config():
        print("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        return
    
    # é‡ç½®çŠ¶æ€é€‰é¡¹
    if args.reset:
        import os
        try:
            os.remove("generation_state.json")
            print("ğŸ”„ å·²é‡ç½®ç”ŸæˆçŠ¶æ€")
        except FileNotFoundError:
            pass
    
    # ä»é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°è·å–è®¾ç½®
    total_conversations = args.total_conversations or config.get('generation.default_total_conversations', 6000)
    batch_size = args.batch_size or config.get('generation.default_batch_size', 50)
    output_file = args.output_file or config.get('generation.default_output_file', "function_calling_dataset_smart.json")
    prompt_file = args.prompt_file or config.get('generation.default_prompt_file', "prompt.txt")
    
    api_key = config.get_api_key()
    print("ğŸš€ å¼€å§‹ä½¿ç”¨æ·±åº¦æ±‚ç´¢APIç”Ÿæˆé—®é¢˜æ•°æ®é›†ï¼ˆæ™ºèƒ½å»é‡ç‰ˆæœ¬ï¼‰...")
    print(f"   API Keyå‰ç¼€: {api_key[:8]}...")
    print("ğŸ§  å¯ç”¨æ™ºèƒ½å»é‡å’Œè¿›åº¦è·Ÿè¸ªåŠŸèƒ½")
    print(f"ğŸ“ ç”Ÿæˆfunction callingé—®é¢˜å’Œå¼•å¯¼å¯¹è¯")
    print(f"   ç›®æ ‡å¯¹è¯æ•°é‡: {total_conversations}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"   æç¤ºè¯æ–‡ä»¶: {prompt_file}")
    
    try:
        generator = SmartQuestionDatasetGenerator(config)
        
        # ç”Ÿæˆé—®é¢˜æ•°æ®é›†
        conversations = await generator.generate_dataset(
            prompt_file=prompt_file,
            total_conversations=total_conversations,
            batch_size=batch_size,
            output_file=output_file
        )
        
        if conversations:
            print(f"âœ… æ™ºèƒ½é—®é¢˜æ•°æ®é›†ç”ŸæˆæˆåŠŸï¼Œå…± {len(conversations)} ä¸ªå¯¹è¯")
            print("ğŸ’¡ ä¸‹ä¸€æ­¥è¿è¡Œ generate_complete_dataset.py æ¥ç”Ÿæˆå®Œæ•´çš„function callingæ•°æ®é›†")
        else:
            print("âŒ é—®é¢˜æ•°æ®é›†ç”Ÿæˆå¤±è´¥")
            
    except ValueError as e:
        print(f"âŒ é…ç½®é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥config.jsonæ–‡ä»¶æˆ–è®¾ç½®ç¯å¢ƒå˜é‡DEEPSEEK_API_KEY")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    asyncio.run(main())